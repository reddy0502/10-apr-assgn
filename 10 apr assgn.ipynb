{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc8dfc0-5c42-4863-803e-37656b8dc266",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Let S be the event that an employee is a smoker, and H be the event that an employee uses the health insurance plan. Then we want to find P(S|H), the probability of S given H.\n",
    "\n",
    "Bayes' theorem states that:\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "where P(H|S) is the probability of using the health insurance plan given that the employee is a smoker, P(S) is the overall probability of being a smoker, and P(H) is the overall probability of using the health insurance plan.\n",
    "\n",
    "From the problem statement, we are given:\n",
    "\n",
    "P(H) = 0.7 \n",
    "P(S|H) = ? \n",
    "P(S) = 0.4 \n",
    "P(H|S) = 0.4 \n",
    "Substituting these values into Bayes' theorem, we get:\n",
    "\n",
    "P(S|H) = P(H|S) * P(S) / P(H)\n",
    "\n",
    "P(S|H) = 0.4 * 0.4 / 0.7\n",
    "\n",
    "P(S|H) = 0.2286 or approximately 0.23\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is about 0.23 or 23%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da81de1-17f7-4c64-a120-d060b5f837c3",
   "metadata": {},
   "source": [
    "2ans:\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes is in how they represent the input features - Bernoulli assumes binary features, while Multinomial assumes count-based features. The choice between these two algorithms depends on the nature of the input features and the specific task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48de8ee-b25f-4a07-9e4a-7799b6a85c4b",
   "metadata": {},
   "source": [
    "3ans:\n",
    "\n",
    "Bernoulli Naive Bayes assumes that each feature is binary, which means that it does not handle missing values explicitly. If a feature value is missing, it is typically treated as if the feature is not present, and the corresponding probability is calculated based on the remaining features.\n",
    "\n",
    "In practice, missing values can be handled by either imputing them with some default value or by treating them as a separate category in the data. However, in the case of Bernoulli Naive Bayes, it is not necessary to handle missing values explicitly, as they are implicitly handled by treating them as if the corresponding feature is not present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a087259-c9e9-4249-ab72-bb9cfd2b3835",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. In Gaussian Naive Bayes, the assumption is that the input features follow a Gaussian (normal) distribution. To perform multi-class classification, the algorithm simply computes the class probabilities for each class, given the input features, using Bayes' theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65f5f2-9fe7-44b7-ba59-8cdaf1f26b9d",
   "metadata": {},
   "source": [
    "5ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
